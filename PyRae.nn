import numpy as np

class Linear:
    """Linear layer, similar to torch.nn.Linear.
    Args:
        in_features (int): Integer representing # input features expected by layer
        out_features (int): Integer representing # features to be output by layer (i.e. # of neurons)
    """
    
    def __init__(self, in_features, out_features):
        bound = np.sqrt(6 / in_features)
        self.weight = np.random.uniform(low=-bound, high=bound, size=(in_features, out_features))
        self.bias = np.zeros(shape=(1, out_features))

        self.grad_weight = np.zeros((in_features, out_features))
        self.grad_bias = np.zeros((1, out_features))

        self.x = None


    def forward(self, x):
        """Forward pass for linear layer.
        Args:
            x (np.array): Input to the layer, shaped (batch_size, in_features)
        Returns:
            np.array: Output of the layer, shaped (batch_size, out_features)
        """
        self.x = x

        return x @ self.weight + self.bias

  
    def backward(self, grad):
        """Backward pass for linear layer.
        Args:
            grad (np.array): The gradient of the loss w.r.t. the output of this layer
                             shaped (batch_size, out_features)
        Returns:
            np.array: The gradient of the loss w.r.t. the input to this layer
                      shaped (batch_size, in_features)
        """
 
        # Calculate and store the gradient of the loss w.r.t. this layer's weight array.
        self.grad_weight = self.x.T @ grad
    
        # Calculate and store the gradient of the loss w.r.t. this layer's bias array.
        self.grad_bias = np.sum(grad, axis=0, keepdims=True)
    
        # Calculate and return the gradient of the loss w.r.t. this layer's input
        return grad @ self.weight.T

        # Calculate and return the gradient of the loss w.r.t. this layer's input
        raise NotImplementedError


class ReLU:
    """The ReLU activation function, similar to `torch.nn.ReLU`."""
    def __init__(self):
        self.x = None

    def forward(self, x):
        """Forward pass for ReLU.
        Args:
            x (np.array): Input shaped (batch_size, *), where * means any number of additional dims.
        Returns:
            np.array: Output, same shape as input (batch_size, *)
        """
        self.x = x

        return np.where(x > 0, x, 0)

    def backward(self, grad):
        """Backward pass for ReLU.
        Args:
            grad (np.array): The gradient of the loss w.r.t. the output of this function
                             shaped (batch_size, *)
        Returns:
            np.array: The gradient of the loss w.r.t. the input to this function
                      shaped (batch_size, *)
        """

        # Use the derivative of ReLU: 1 for x > 0, otherwise 0
        relu_grad = np.where(self.x > 0, 1, 0)
    
        # Element-wise multiply the incoming gradient by the ReLU gradient
        return grad * relu_grad


class Sequential:
    """Takes given layers and makes a simple feed-forward network from them. Similar to `torch.nn.Sequential`
    Accepts any number of layers.

    Example:
    >>> model = Sequential(nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 5))
    The input to model must be shaped (batch_size, 3)
    The output of model will be shaped (batch_size, 5)
    """

    def __init__(self, *layers):
        self.layers = list(layers) # Stores layers in a list, ex) [nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 5)]

    def forward(self, x):
        """Passes input `x` through each of the layers in order, returns final output.
        Args:
            x (np.array): Input shaped (batch_size, num_features)
                          Must be shaped appropriately to go in first layer.
        Returns:
            np.array: Output after passing through all layers shaped (batch_size, num_classes)
        """
  
        # Pass input through the network, return final output
        for layer in self.layers:
            x = layer.forward(x)
        return x

    def backward(self, loss_function):
        """Runs backpropagation. Does not return anything.
        Args:
            loss_function (nn.CrossEntropyLoss): Loss function after running the forward pass (input/target already stored)
        """
    
        # Step 1: Get the initial gradient from the loss function's backward method
        grad = loss_function.backward()

        # Step 2: Pass the gradient through each layer in reverse order
        for layer in reversed(self.layers):
            grad = layer.backward(grad)


def softmax(x):
    """[Given] Calculates the softmax of the input array using the LogSumExp trick for numerical stability.
    Args:
        x (np.array): Input array, shaped (batch_size, d), where d is any integer.
    Returns:
        np.array: Same shape as input, but the values of each row are now scaled to add up to 1.
    """
 
    # Subtract max for numerical stability
    a = np.max(x, axis=1, keepdims=True)
    exp_x = np.exp(x - a)  # Numerator

    # Denominator calculation with LogSumExp trick
    denom = np.sum(exp_x, axis=1, keepdims=True)
    return exp_x / denom


def make_one_hot(idx_labels, num_classes=10):
    """[Given] Converts index labels to one-hot encoding.

    >>> make_one_hot(np.array([0, 2, 1]), num_classes=3)
    array([[1., 0., 0.],
           [0., 0., 1.],
           [0., 1., 0.]])

    Args:
        idx_labels (np.array): Array of labels
        num_classes (int, optional): # of possible classes there are.

    Returns:
        np.array: One-hot encoded labels.
    """

    # [Given] Used in cross entropy loss
    one_hot_labels = np.zeros((len(idx_labels), num_classes))
    one_hot_labels[np.arange(len(idx_labels)), idx_labels] = 1
    return one_hot_labels


class CrossEntropyLoss:
    """Cross-entropy loss function. Similar to `torch.nn.CrossEntropyLoss`."""

    def __init__(self):
        self.input = None
        self.target = None

    def forward(self, input, target):
        """Calculates the average loss of the batched inputs and targets.
        Args:
            input (np.array): Logits (output of model) of shape (batch_size, num_classes)
            target (np.array): Label array of shape (batch_size,), after conversion to one-hot it's shaped (batch_size, num_classes)
        Returns:
            float: The loss value (averaged across the batch)
        """
        
        # Convert the targets to a one-hot encoding shaped (batch_size, num_classes)
        target = make_one_hot(target, num_classes=input.shape[1])
        
        # Store the inputs and the one-hot encoded targets for backward
        self.input = input
        self.target = target
        
        # Compute softmax probabilities
        probs = softmax(input)
        
        # Calculate the cross-entropy loss
        log_probs = np.log(probs + 1e-12)  # Add small value for numerical stability
        loss = -np.sum(target * log_probs) / input.shape[0]
        
        return loss

    def backward(self):
        """Begins backprop by calculating the gradient of the loss w.r.t. CrossEntropyLoss's forward.
        Similar to calling `loss.backward()` in Torch.
        Returns:
            np.array: the gradient of the loss w.r.t. the input of CrossEntropyLoss (batch size, num_classes)
        """
        
        # Calculate softmax probabilities
        probs = softmax(self.input)
        
        # Gradient of cross-entropy loss w.r.t. input (logits)
        grad = (probs - self.target) / self.input.shape[0]
        
        return grad
